{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_pytorch import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# One of:\n",
    "#     'state-spaces/mamba-2.8b-slimpj'\n",
    "#     'state-spaces/mamba-2.8b'\n",
    "#     'state-spaces/mamba-1.4b'\n",
    "#     'state-spaces/mamba-790m'\n",
    "#     'state-spaces/mamba-370m'\n",
    "#     'state-spaces/mamba-130m'\n",
    "pretrained_model_name = 'state-spaces/mamba-370m'\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Mamba as MambaJax\n",
    "from model import ModelArgs as ModelArgsJax\n",
    "\n",
    "modelArgsJax = ModelArgsJax(\n",
    "    d_model=model.args.d_model,\n",
    "    n_layer=model.args.n_layer,\n",
    "    vocab_size=model.args.vocab_size,\n",
    ")\n",
    "\n",
    "modelJax = MambaJax(modelArgsJax)\n",
    "\n",
    "from params import pytorch_to_jax_weights\n",
    "\n",
    "weights = pytorch_to_jax_weights(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jax(model,\n",
    "             params,\n",
    "             tokenizer,\n",
    "             prompt: str,\n",
    "             n_tokens_to_gen: int = 50,\n",
    "             sample: bool = True,\n",
    "             top_k: int = 40,\n",
    "             temperature: float = 1.0,\n",
    "             seed: int = 0):\n",
    "    \n",
    "    # @jax.jit\n",
    "    def model_inference(params, input_ids):\n",
    "        return model.apply({\"params\": params}, input_ids)\n",
    "\n",
    "    initial_input_ids = jnp.array(tokenizer(prompt, return_tensors='jax').input_ids)\n",
    "    max_len = initial_input_ids.shape[1] + n_tokens_to_gen\n",
    "    input_ids = jnp.pad(initial_input_ids, ((0, 0), (0, n_tokens_to_gen)))\n",
    "    layer_outputs = None\n",
    "    \n",
    "    # @jax.jit\n",
    "    def generate_step(state):\n",
    "        input_ids, current_pos, rng = state\n",
    "        next_token_logits = model_inference(params, input_ids[:, :current_pos])[:, -1, :]\n",
    "        \n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        probs = jax.nn.softmax(next_token_logits, axis=-1)\n",
    "        \n",
    "        if top_k is not None:\n",
    "            top_k_probs, top_k_indices = jax.lax.top_k(probs, k=top_k)\n",
    "            probs = jnp.zeros_like(probs).at[jnp.arange(probs.shape[0])[:, None], top_k_indices].set(top_k_probs)\n",
    "            probs = probs / probs.sum(axis=-1, keepdims=True)\n",
    "        \n",
    "        if sample:\n",
    "            rng, next_rng = random.split(rng)\n",
    "            next_index = random.categorical(next_rng, probs, axis=-1)\n",
    "        else:\n",
    "            next_index = jnp.argmax(probs, axis=-1)\n",
    "        \n",
    "        input_ids = input_ids.at[:, current_pos].set(next_index)\n",
    "        current_pos += 1\n",
    "        \n",
    "        return input_ids, current_pos, rng\n",
    "\n",
    "    rng = random.PRNGKey(seed)\n",
    "    initial_pos = initial_input_ids.shape[1]\n",
    "    # final_state = jax.lax.while_loop(cond_fn, generate_step, (input_ids, initial_pos, rng))\n",
    "    for _ in tqdm(range(n_tokens_to_gen)):\n",
    "        input_ids, initial_pos, rng = generate_step((input_ids, initial_pos, rng))\n",
    "\n",
    "\n",
    "    # final_input_ids, _, _ = final_state\n",
    "    final_input_ids = input_ids\n",
    "    output_completions = tokenizer.decode(final_input_ids[0, :max_len].tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pytorch(model,\n",
    "             tokenizer,\n",
    "             prompt: str,\n",
    "             n_tokens_to_gen: int = 50,\n",
    "             sample: bool = True,\n",
    "             top_k: int = 40,):\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    layer_outputs = None\n",
    "    \n",
    "    for token_n in range(n_tokens_to_gen):\n",
    "        with torch.no_grad():\n",
    "            indices_to_input = input_ids\n",
    "            next_token_logits = model(indices_to_input)[:, -1]\n",
    "        \n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        (batch, vocab_size) = probs.shape\n",
    "        \n",
    "        if top_k is not None:\n",
    "            (values, indices) = torch.topk(probs, k=top_k)\n",
    "            probs[probs < values[:, -1, None]] = 0\n",
    "            probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        if sample:\n",
    "            next_indices = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "    \n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"The quick brown fox jumps\"\n",
    "prompt = \"Mamba is the\"\n",
    "\n",
    "output= generate_jax(modelJax, weights, tokenizer, prompt, seed=0, n_tokens_to_gen=10, sample=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Mamba is the\"\n",
    "\n",
    "output_pytorch = generate_pytorch(model, tokenizer, prompt, sample=False, n_tokens_to_gen=10)\n",
    "output_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
